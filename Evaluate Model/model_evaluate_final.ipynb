{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/sefat/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sefat/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sefat/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sefat/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sefat/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sefat/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/sefat/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sefat/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sefat/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sefat/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sefat/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sefat/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2430, 6)\n",
      "(2430,)\n",
      "WARNING:tensorflow:From /home/sefat/.local/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/sefat/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Fold number :1\n",
      "Precision: 0.352459\n",
      "Recall: 1.000000\n",
      "F1 score: 0.521212\n",
      "accuracy: 64.75%\n",
      "Fold number :2\n",
      "Precision: 0.539823\n",
      "Recall: 0.709302\n",
      "F1 score: 0.613065\n",
      "accuracy: 72.54%\n",
      "Fold number :3\n",
      "Precision: 0.600000\n",
      "Recall: 0.627907\n",
      "F1 score: 0.613636\n",
      "accuracy: 74.18%\n",
      "Fold number :4\n",
      "Precision: 0.578947\n",
      "Recall: 0.647059\n",
      "F1 score: 0.611111\n",
      "accuracy: 73.25%\n",
      "Fold number :5\n",
      "Precision: 0.349794\n",
      "Recall: 1.000000\n",
      "F1 score: 0.518293\n",
      "accuracy: 65.02%\n",
      "Fold number :6\n",
      "Precision: 0.465116\n",
      "Recall: 0.705882\n",
      "F1 score: 0.560748\n",
      "accuracy: 72.43%\n",
      "Fold number :7\n",
      "Precision: 0.509615\n",
      "Recall: 0.623529\n",
      "F1 score: 0.560847\n",
      "accuracy: 71.19%\n",
      "Fold number :8\n",
      "Precision: 0.524272\n",
      "Recall: 0.635294\n",
      "F1 score: 0.574468\n",
      "accuracy: 69.01%\n",
      "Fold number :9\n",
      "Precision: 0.504950\n",
      "Recall: 0.600000\n",
      "F1 score: 0.548387\n",
      "accuracy: 66.94%\n",
      "Fold number :10\n",
      "Precision: 0.544643\n",
      "Recall: 0.717647\n",
      "F1 score: 0.619289\n",
      "accuracy: 74.79%\n",
      "70.41% \n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from bangla_stemmer import *\n",
    "from collections import Counter\n",
    "from math import log\n",
    "import os\n",
    "import numpy\n",
    "import operator\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_dim=6, units=10, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(units=4, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(units=10, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(units=4, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(units=1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def list_common_words(doc1, doc2):\n",
    "    words = []\n",
    "    for word in doc1:\n",
    "        if word in doc2:\n",
    "            words.append(word)\n",
    "    return words\n",
    "\n",
    "def get_embedding(sentence):\n",
    "    model =doc2vec.load('/home/sefat/PycharmProjects/SPL3/doc2vec.model')\n",
    "    return model.infer_vector(sentence)\n",
    "\n",
    "def cosine_value(line0,line1,texts):\n",
    "    vocabul =[]\n",
    "    term, inverse = dict(), dict()\n",
    "    N = len(texts)\n",
    "    for text in texts:\n",
    "        for word in text:\n",
    "            vocabul.append(word)\n",
    "    for word in vocabul:\n",
    "        term[word] = vocabul.count(word)\n",
    "        n = 0\n",
    "        for text in texts:\n",
    "            if word in text:\n",
    "                n += 1\n",
    "        inverse[word] = log10(float(N / n))\n",
    "\n",
    "    words =list_common_words(line0,line1)\n",
    "    numerator =0\n",
    "    for word in words:\n",
    "        numerator += term[word] * term[word] * inverse[word] * inverse[word]\n",
    "    denominator1, denominator2 = 0, 0\n",
    "    for word in line0:\n",
    "        denominator1 += term[word] * term[word] * inverse[word] * inverse[word]\n",
    "    for word in line1:\n",
    "        denominator2 += term[word] * term[word] * inverse[word] * inverse[word]\n",
    "    denominator1 = sqrt(denominator1)\n",
    "    denominator2 = sqrt(denominator2)\n",
    "    denominator = denominator1 * denominator2\n",
    "    return float(numerator/denominator)\n",
    "\n",
    "def method(matrix, iterations, tolerance):\n",
    "    eigen_vector = numpy.zeros(shape=(len(matrix)))\n",
    "    eigen_vector = (eigen_vector + 1) / len(matrix)\n",
    "    matrix = numpy.array(matrix)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        new_vector = numpy.matmul(eigen_vector, matrix)\n",
    "        if sum(numpy.absolute(new_vector - eigen_vector)) < tolerance:\n",
    "            break\n",
    "        eigen_vector = new_vector\n",
    "\n",
    "    return eigen_vector\n",
    "\n",
    "def common_words(tittle,sentence):\n",
    "    counter =0\n",
    "    for word in tittle:\n",
    "        if word in sentence:\n",
    "            counter+=1\n",
    "    return counter\n",
    "\n",
    "def calculate_top_words(top_words,sentence):\n",
    "    count =0\n",
    "    for word in sentence:\n",
    "        if word in top_words:\n",
    "            count+=1\n",
    "    return count\n",
    "\n",
    "def generate_degree(texts):\n",
    "    #texts = pre_process(texts)\n",
    "    vocabul = []\n",
    "    term, inverse = dict(), dict()\n",
    "    N = len(texts)\n",
    "    for text in texts:\n",
    "        for word in text:\n",
    "            vocabul.append(word)\n",
    "    for word in vocabul:\n",
    "        term[word] = vocabul.count(word)\n",
    "        n = 0\n",
    "        for text in texts:\n",
    "            if word in text:\n",
    "                n += 1\n",
    "        inverse[word] = log10(float(N / n))\n",
    "\n",
    "    matrix = []\n",
    "    for i in range(len(texts)):\n",
    "        matrix.append([0] * len(texts))\n",
    "\n",
    "    for i in range(len(texts)):\n",
    "        for j in range(len(texts)):\n",
    "            words = list_common_words(texts[i], texts[j])\n",
    "            numerator = 0\n",
    "            for word in words:\n",
    "                numerator += term[word] * term[word] * inverse[word] * inverse[word]\n",
    "            denominator1, denominator2 = 0, 0\n",
    "            for word in texts[i]:\n",
    "                denominator1 += term[word] * term[word] * inverse[word] * inverse[word]\n",
    "            for word in texts[j]:\n",
    "                denominator2 += term[word] * term[word] * inverse[word] * inverse[word]\n",
    "            denominator1 = sqrt(denominator1)\n",
    "            denominator2 = sqrt(denominator2)\n",
    "            denominator = denominator1 * denominator2\n",
    "            matrix[i][j] = float(numerator / denominator)\n",
    "\n",
    "    threshold = 0.2\n",
    "    degrees = [0] * len(matrix)\n",
    "\n",
    "    for i in range(len(matrix)):\n",
    "        for j in range(len(matrix[i])):\n",
    "            if matrix[i][j] >= threshold:\n",
    "                degrees[i] += 1\n",
    "\n",
    "    for i in range(len(matrix)):\n",
    "        for j in range(len(matrix)):\n",
    "            if matrix[i][j] >= threshold:\n",
    "                matrix[i][j] = 1\n",
    "                continue\n",
    "            matrix[i][j] = 0\n",
    "\n",
    "    for i in range(len(matrix)):\n",
    "        for j in range(len(matrix)):\n",
    "            matrix[i][j] = matrix[i][j] / degrees[i]\n",
    "\n",
    "    scores = method(matrix, 50, 0.01)\n",
    "    #scores[0] = 1\n",
    "    return scores\n",
    "\n",
    "def calculate_degree(sentence,document,index):\n",
    "    degree =0\n",
    "    threshold =0.010\n",
    "    similarities =[0]*len(document)\n",
    "    for i in range(len(document)):\n",
    "        try:\n",
    "            similarities[i] = common_words(sentence, document[i]) / (log(len(sentence)) + log(len(document[i])))\n",
    "        except:\n",
    "            similarities[i] =0.0\n",
    "    similarities[index] =0.0\n",
    "    for i in range(len(document)):\n",
    "        if similarities[i] >= threshold:\n",
    "            degree+=1\n",
    "    return degree\n",
    "\n",
    "def calculate_sentence_freq(sentence,vocab):\n",
    "    total_freq =0\n",
    "    for word in sentence:\n",
    "        total_freq+=vocab[word]\n",
    "    return total_freq-len(sentence)\n",
    "\n",
    "# doc_path =os.getcwd()+'/Dataset/Train/Documents'\n",
    "# sum_path =os.getcwd()+'/Dataset/Train/Summaries'\n",
    "\n",
    "doc_path ='/home/sefat/PycharmProjects/SPL3/Dataset/Train/Documents'\n",
    "sum_path ='/home/sefat/PycharmProjects/SPL3/Dataset/Train/Summaries'\n",
    "\n",
    "parser =RuleFileParser()\n",
    "\n",
    "chars_to_delete = \"!@#$%^&*(){}[]<>?,./+-_|''\\\\\\\"\\/।\"\n",
    "features =[]\n",
    "target_values =[]\n",
    "stopwords =[\"অতএব\",\"অথচ\",\"অথবা\",\"অনুযায়ী\",\"অনেক\",\"অনেকে\",\"অনেকেই\",\"অন্তত\",\"অন্য\",\"অবধি\",\"অবশ্য\",\"অর্থাত\",\"আই\",\"আগামী\",\"আগে\",\"আগেই\",\"আছে\",\"আজ\",\"আদ্যভাগে\",\"আপনার\",\"আপনি\",\"আবার\",\"আমরা\",\"আমাকে\",\"আমাদের\",\"আমার\",\"আমি\",\"আর\",\"আরও\",\"ই\",\"ইত্যাদি\",\"ইহা\",\"উচিত\",\"উত্তর\",\"উনি\",\"উপর\",\"উপরে\",\"এ\",\"এঁদের\",\"এঁরা\",\"এই\",\"একই\",\"একটি\",\"একবার\",\"একে\",\"এক্\",\"এখন\",\"এখনও\",\"এখানে\",\"এখানেই\",\"এটা\",\"এটাই\",\"এটি\",\"এত\",\"এতটাই\",\"এতে\",\"এদের\",\"এব\",\"এবং\",\"এবার\",\"এমন\",\"এমনকী\",\"এমনি\",\"এর\",\"এরা\",\"এল\",\"এস\",\"এসে\",\"ঐ\",\"ও\",\"ওঁদের\",\"ওঁর\",\"ওঁরা\",\"ওই\",\"ওকে\",\"ওখানে\",\"ওদের\",\"ওর\",\"ওরা\",\"কখনও\",\"কত\",\"কবে\",\"কমনে\",\"কয়েক\",\"কয়েকটি\",\"করছে\",\"করছেন\",\"করতে\",\"করবে\",\"করবেন\",\"করলে\",\"করলেন\",\"করা\",\"করাই\",\"করায়\",\"করার\",\"করি\",\"করিতে\",\"করিয়া\",\"করিয়ে\",\"করে\",\"করেই\",\"করেছিলেন\",\"করেছে\",\"করেছেন\",\"করেন\",\"কাউকে\",\"কাছ\",\"কাছে\",\"কাজ\",\"কাজে\",\"কারও\",\"কারণ\",\"কি\",\"কিংবা\",\"কিছু\",\"কিছুই\",\"কিন্তু\",\"কী\",\"কে\",\"কেউ\",\"কেউই\",\"কেখা\",\"কেন\",\"কোটি\",\"কোন\",\"কোনও\",\"কোনো\",\"ক্ষেত্রে\",\"কয়েক\",\"খুব\",\"গিয়ে\",\"গিয়েছে\",\"গিয়ে\",\"গুলি\",\"গেছে\",\"গেল\",\"গেলে\",\"গোটা\",\"চলে\",\"চান\",\"চায়\",\"চার\",\"চালু\",\"চেয়ে\",\"চেষ্টা\",\"ছাড়া\",\"ছাড়াও\",\"ছিল\",\"ছিলেন\",\"জন\",\"জনকে\",\"জনের\",\"জন্য\",\"জন্যওজে\",\"জানতে\",\"জানা\",\"জানানো\",\"জানায়\",\"জানিয়ে\",\"জানিয়েছে\",\"জে\",\"জ্নজন\",\"টি\",\"ঠিক\",\"তখন\",\"তত\",\"তথা\",\"তবু\",\"তবে\",\"তা\",\"তাঁকে\",\"তাঁদের\",\"তাঁর\",\"তাঁরা\",\"তাঁাহারা\",\"তাই\",\"তাও\",\"তাকে\",\"তাতে\",\"তাদের\",\"তার\",\"তারপর\",\"তারা\",\"তারৈ\",\"তাহলে\",\"তাহা\",\"তাহাতে\",\"তাহার\",\"তিনঐ\",\"তিনি\",\"তিনিও\",\"তুমি\",\"তুলে\",\"তেমন\",\"তো\",\"তোমার\",\"থাকবে\",\"থাকবেন\",\"থাকা\",\"থাকায়\",\"থাকে\",\"থাকেন\",\"থেকে\",\"থেকেই\",\"থেকেও\",\"দিকে\",\"দিতে\",\"দিন\",\"দিয়ে\",\"দিয়েছে\",\"দিয়েছেন\",\"দিলেন\",\"দু\",\"দুই\",\"দুটি\",\"দুটো\",\"দেওয়া\",\"দেওয়ার\",\"দেওয়া\",\"দেখতে\",\"দেখা\",\"দেখে\",\"দেন\",\"দেয়\",\"দ্বারা\",\"ধরা\",\"ধরে\",\"ধামার\",\"নতুন\",\"নয়\",\"না\",\"নাই\",\"নাকি\",\"নাগাদ\",\"নানা\",\"নিজে\",\"নিজেই\",\"নিজেদের\",\"নিজের\",\"নিতে\",\"নিয়ে\",\"নিয়ে\",\"নেই\",\"নেওয়া\",\"নেওয়ার\",\"নেওয়া\",\"নয়\",\"পক্ষে\",\"পর\",\"পরে\",\"পরেই\",\"পরেও\",\"পর্যন্ত\",\"পাওয়া\",\"পাচ\",\"পারি\",\"পারে\",\"পারেন\",\"পি\",\"পেয়ে\",\"পেয়্র্\",\"প্রতি\",\"প্রথম\",\"প্রভৃতি\",\"প্রযন্ত\",\"প্রাথমিক\",\"প্রায়\",\"প্রায়\",\"ফলে\",\"ফিরে\",\"ফের\",\"বক্তব্য\",\"বদলে\",\"বন\",\"বরং\",\"বলতে\",\"বলল\",\"বললেন\",\"বলা\",\"বলে\",\"বলেছেন\",\"বলেন\",\"বসে\",\"বহু\",\"বা\",\"বাদে\",\"বার\",\"বি\",\"বিনা\",\"বিভিন্ন\",\"বিশেষ\",\"বিষয়টি\",\"বেশ\",\"বেশি\",\"ব্যবহার\",\"ব্যাপারে\",\"ভাবে\",\"ভাবেই\",\"মতো\",\"মতোই\",\"মধ্যভাগে\",\"মধ্যে\",\"মধ্যেই\",\"মধ্যেও\",\"মনে\",\"মাত্র\",\"মাধ্যমে\",\"মোট\",\"মোটেই\",\"যখন\",\"যত\",\"যতটা\",\"যথেষ্ট\",\"যদি\",\"যদিও\",\"যা\",\"যাঁর\",\"যাঁরা\",\"যাওয়া\",\"যাওয়ার\",\"যাওয়া\",\"যাকে\",\"যাচ্ছে\",\"যাতে\",\"যাদের\",\"যান\",\"যাবে\",\"যায়\",\"যার\",\"যারা\",\"যিনি\",\"যে\",\"যেখানে\",\"যেতে\",\"যেন\",\"যেমন\",\"র\",\"রকম\",\"রয়েছে\",\"রাখা\",\"রেখে\",\"লক্ষ\",\"শুধু\",\"শুরু\",\"সঙ্গে\",\"সঙ্গেও\",\"সব\",\"সবার\",\"সমস্ত\",\"সম্প্রতি\",\"সহ\",\"সহিত\",\"সাধারণ\",\"সামনে\",\"সি\",\"সুতরাং\",\"সে\",\"সেই\",\"সেখান\",\"সেখানে\",\"সেটা\",\"সেটাই\",\"সেটাও\",\"সেটি\",\"স্পষ্ট\",\"স্বয়ং\",\"হইতে\",\"হইবে\",\"হইয়া\",\"হওয়া\",\"হওয়ায়\",\"হওয়ার\",\"হচ্ছে\",\"হত\",\"হতে\",\"হতেই\",\"হন\",\"হবে\",\"হবেন\",\"হয়\",\"হয়তো\",\"হয়নি\",\"হয়ে\",\"হয়েই\",\"হয়েছিল\",\"হয়েছে\",\"হয়েছেন\",\"হল\",\"হলে\",\"হলেই\",\"হলেও\",\"হলো\",\"হিসাবে\",\"হৈলে\",\"হোক\",\"হয়\"]\n",
    "\n",
    "for file_index in range(1,201):\n",
    "    doc_lines =open(doc_path+'/Document_'+str(file_index)+'.txt','r',encoding='utf-8').readlines()\n",
    "    sum_lines =open(sum_path+'/Document_'+str(file_index)+'_Summary_1'+'.txt','r',encoding='utf-8').readlines()\n",
    "\n",
    "    for i in range(len(doc_lines)):\n",
    "        for char in chars_to_delete:\n",
    "            while True:\n",
    "                index =doc_lines[i].find(char)\n",
    "                if index == -1:\n",
    "                    break\n",
    "                doc_lines[i] = doc_lines[i][:index]+doc_lines[i][index+1:]\n",
    "\n",
    "    for i in range(len(sum_lines)):\n",
    "        for char in chars_to_delete:\n",
    "            while True:\n",
    "                index =sum_lines[i].find(char)\n",
    "                if index == -1:\n",
    "                    break\n",
    "                sum_lines[i] =sum_lines[i][:index]+sum_lines[i][index+1:]\n",
    "\n",
    "    for i in range(len(doc_lines)):\n",
    "        doc_lines[i] =doc_lines[i].split()\n",
    "\n",
    "    for i in range(len(sum_lines)):\n",
    "        sum_lines[i] =sum_lines[i].split()\n",
    "\n",
    "    for i in range(len(doc_lines)):\n",
    "        new_doc_line =[]\n",
    "        for j in range(len(doc_lines[i])):\n",
    "            if doc_lines[i][j] not in stopwords and len(doc_lines[i][j]) > 1:\n",
    "                new_doc_line.append(doc_lines[i][j])\n",
    "        doc_lines[i] =new_doc_line\n",
    "\n",
    "        for j in range(len(doc_lines[i])):\n",
    "            doc_lines[i][j] =parser.stemOfWord(doc_lines[i][j])\n",
    "\n",
    "    for i in range(len(sum_lines)):\n",
    "        new_sum_line =[]\n",
    "        for j in range(len(sum_lines[i])):\n",
    "            if sum_lines[i][j] not in stopwords and len(sum_lines[i][j]) > 1:\n",
    "                new_sum_line.append(sum_lines[i][j])\n",
    "        sum_lines[i] =new_sum_line\n",
    "\n",
    "        for j in range(len(sum_lines[i])):\n",
    "            sum_lines[i][j] =parser.stemOfWord(sum_lines[i][j])\n",
    "\n",
    "    tittle =doc_lines[1]\n",
    "    #doc_lines =doc_lines[1:]\n",
    "\n",
    "    targets =[0]*len(doc_lines)\n",
    "\n",
    "    for i in range(len(doc_lines)):\n",
    "        for j in range(len(sum_lines)):\n",
    "            flag =True\n",
    "            for k in range(len(doc_lines[i])):\n",
    "                if doc_lines[i][k] not in sum_lines[j]:\n",
    "                    flag =False\n",
    "                    break\n",
    "            if flag:\n",
    "                targets[i] =1\n",
    "                break\n",
    "    target_values.extend(targets)\n",
    "\n",
    "    lengths =[]\n",
    "    positions =[]\n",
    "    similar_words =[]\n",
    "    degrees =[]\n",
    "    sen_freq =[]\n",
    "    keywords =[]\n",
    "    vocab =[]\n",
    "\n",
    "    for line in doc_lines:\n",
    "        vocab.extend(line)\n",
    "\n",
    "    vocab =Counter(vocab)\n",
    "\n",
    "    sorted_x = sorted(vocab.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "    top_words =[]\n",
    "\n",
    "    for i in range(20):\n",
    "        top_words.append(sorted_x[i][0])\n",
    "\n",
    "    # print(top_words)\n",
    "\n",
    "    #COLLECTING FEATURES\n",
    "\n",
    "    for i in range(len(doc_lines)):\n",
    "        lengths.append(len(doc_lines[i]))\n",
    "        positions.append(1-(i+1)/len(doc_lines))\n",
    "        similar_words.append(common_words(tittle, doc_lines[i]))\n",
    "        degrees.append(calculate_degree(doc_lines[i],doc_lines,i))\n",
    "        sen_freq.append(calculate_sentence_freq(doc_lines[i],vocab))\n",
    "        keywords.append(calculate_top_words(top_words,doc_lines[i]))\n",
    "\n",
    "    for i in range(len(doc_lines)):\n",
    "        features.append([lengths[i],positions[i],similar_words[i],degrees[i],sen_freq[i],keywords[i]])\n",
    "\n",
    "# CONVERTING TO NUMPY ARRAY\n",
    "\n",
    "features =numpy.array(features)\n",
    "print(features.shape)\n",
    "target_values =numpy.array(target_values)\n",
    "print(target_values.shape)\n",
    "\n",
    "scalar =MinMaxScaler()\n",
    "features =scalar.fit_transform(features)\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from  sklearn.model_selection import StratifiedKFold\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "cvscores = []\n",
    "fold_number =1\n",
    "for train, test in kfold.split(features, target_values):\n",
    "    model =create_model()\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(features[train], target_values[train], epochs=100, batch_size=10, verbose=0)\n",
    "    scores = model.evaluate(features[test], target_values[test], verbose=2)\n",
    "    y_actual =numpy.array(target_values[test])\n",
    "    y_predict =model.predict(features[test])\n",
    "    # for i in y_predict:\n",
    "    #     print(i)\n",
    "    # print()\n",
    "    # print()\n",
    "    y_predict =y_predict[:,0]\n",
    "    threshold =0.30\n",
    "    for i in range(len(y_predict)):\n",
    "        if y_predict[i] >= threshold:\n",
    "            y_predict[i] =1\n",
    "        else:\n",
    "            y_predict[i] =0\n",
    "\n",
    "    print('Fold number :'+str(fold_number))\n",
    "    precision = precision_score(y_actual, y_predict)\n",
    "    print('Precision: %f' % precision)\n",
    "    recall = recall_score(y_actual,y_predict)\n",
    "    print('Recall: %f' % recall)\n",
    "    f1 = f1_score(y_actual,y_predict)\n",
    "    print('F1 score: %f' % f1)\n",
    "    fold_number+=1\n",
    "\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"%.2f%% \" % (numpy.mean(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_f1_score =0.521212+0.613636+0.613636+0.611111+0.518293+0.560748+0.623529+0.635294+0.548387+0.619289\n",
    "average_f1_score =float(total_f1_score/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5865135\n"
     ]
    }
   ],
   "source": [
    "print(average_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
